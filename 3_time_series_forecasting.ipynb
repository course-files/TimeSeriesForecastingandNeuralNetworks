{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gKSOfs5Hor9"
   },
   "source": [
    "# Time Series Analysis, Time Series Forecasting, and Neural Networks"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Key              | Value                                                                                                                                                                                                                                                                                                            |\n",
    "|:-----------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Course Codes** | BBT 4206 and BFS 4102                                                                                                                                                                                                                                                                                            |\n",
    "| **Course Names** | BBT 4206: Business Intelligence II (Week 7-9 of 13)<br>BFS 4102: Advanced Business Data Analytics (Week 7-9 of 13)                                                                                                                                                                                               |\n",
    "| **Semester**     | August to November 2025                                                                                                                                                                                                                                                                                          |\n",
    "| **Lecturer**     | Allan Omondi                                                                                                                                                                                                                                                                                                     |\n",
    "| **Contact**      | aomondi@strathmore.edu                                                                                                                                                                                                                                                                                           |\n",
    "| **Note**         | The lecture contains both theory and practice.<br/>This notebook forms part of the practice.<br/>It is intended for educational purposes only.<br/>Recommended citation: [BibTex](https://raw.githubusercontent.com/course-files/TimeSeriesForecastingandNeuralNetworks/refs/heads/main/RecommendedCitation.bib) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvIc_BGtHosB"
   },
   "source": "## Stationarity in Time Series"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In-Built Stationarizing Mechanism in Time Series Forecasting Algorithms\n",
    "1. **❌Exponential Smoothing (Simple, Double [Holt], and Triple [Holt-Winters])**\n",
    "- Exponential Smoothing techniques do not require the series to be stationary.\n",
    "- Instead, they explicitly model components such as trend and seasonality directly.\n",
    "- They handle non-stationarity by design, rather than transforming the data to a stationary form.\n",
    "\n",
    "2. **✅ARIMA (AutoRegressive Integrated Moving Average)**\n",
    "- ARIMA explicitly includes differencing (the \"I\" in ARIMA) as part of the model to make the series stationary.\n",
    "- You specify the `d` parameter in `ARIMA(p, d, q)`, where:\n",
    "  - `d = 1` tells the model to difference the data once. This step transforms a non-stationary series into a stationary one internally.\n",
    "\n",
    "3. **✅SARIMA (Seasonal ARIMA)**\n",
    "- SARIMA extends ARIMA with seasonal differencing.\n",
    "- It uses two differencing parameters:\n",
    "  - `d` for non-seasonal differencing,\n",
    "  - `D` for seasonal differencing over a defined period m.\n",
    "- Therefore, SARIMA also has built-in mechanisms to induce both stationarity and seasonal stationarity.\n",
    "\n",
    "4. **❌LSTM (Long Short-Term Memory Networks)**\n",
    "- LSTM models do not have explicit stationarizing mechanisms.\n",
    "- They learn directly from raw sequences, even if they are non-stationary.\n",
    "- However, data preprocessing such as normalization and differencing often improves performance.\n",
    "- LSTMs can model complex non-linear and non-stationary behavior, but they do not automatically make the data stationary."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Import Libraries"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For file and system operations\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# For data download\n",
    "import yfinance as yf\n",
    "\n",
    "# For time series analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas_datareader as pdr\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "\n",
    "# For time series forecasting\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "\n",
    "## For time series forecasting with LSTM\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# For suppressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "# Ensures that matplotlib plots are displayed directly within the notebook,\n",
    "# right below the code cell that produces them. It makes visualizations appear\n",
    "# inline for easier viewing and analysis.\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2: Load and Preprocess/Clean the Data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_path = './data/stockprice_cleaned.csv'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"❌ Please create the dataset first using the previous Jupyter Notebook.\")\n",
    "else:\n",
    "    print(\"✅ Dataset exists locally\")\n",
    "\n",
    "data = pd.read_csv(dataset_path, encoding='utf-8', nrows=200000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocess/Clean the Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Ensure datetime index and sorted\n",
    "\n",
    "# This line converts the `Date` column in the `data` DataFrame to datetime\n",
    "# objects. The `errors=\"coerce\"` argument means that any value that cannot\n",
    "# be parsed as a date will be replaced with `NaT` (Not a Time), which is\n",
    "# pandas' way of marking missing or invalid dates. This helps prevent errors\n",
    "# from invalid date formats and ensures the column is consistently typed.\n",
    "if \"Date\" in data.columns:\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors=\"coerce\")\n",
    "    data = data.set_index(\"Date\")\n",
    "\n",
    "data = data.sort_index()\n",
    "\n",
    "# 2) Normalize column names (trim, unify, and fix common naming)\n",
    "data.columns = [c.strip() for c in data.columns]\n",
    "rename_map = {\n",
    "    \"Close/Last\": \"Close\",\n",
    "    \"Adj Close\": \"Close\",\n",
    "    \"Adj. Close\": \"Close\",\n",
    "    \"Closing Price\": \"Close\",\n",
    "    \"Open Price\": \"Open\",\n",
    "    \"Opening Price\": \"Open\",\n",
    "    \"High Price\": \"High\",\n",
    "    \"Low Price\": \"Low\",\n",
    "    \"Trade Volume\": \"Volume\",\n",
    "    \"Volume (Shares)\": \"Volume\",\n",
    "    \"Total Volume\": \"Volume\",\n",
    "    \"High/Low\": \"High\",\n",
    "    \"Low/High\": \"Low\"\n",
    "}\n",
    "data = data.rename(columns=rename_map)\n",
    "\n",
    "# 3) Define a helper to strip non-numeric characters (keep digits, dot, minus)\n",
    "def to_numeric_str(s: str) -> str:\n",
    "    # Handle None/NaN safely\n",
    "    s = \"\" if s is None else str(s)\n",
    "    # Strip any characters except digits, decimal point, minus\n",
    "    return re.sub(r\"[^0-9.\\-]\", \"\", s)\n",
    "\n",
    "# 4) Coerce OHLC to float\n",
    "ohlc_cols = [c for c in [\"Open\", \"High\", \"Low\", \"Close\"] if c in data.columns]\n",
    "for c in ohlc_cols:\n",
    "    data[c] = pd.to_numeric(data[c].astype(str).map(to_numeric_str), errors=\"coerce\")\n",
    "\n",
    "# 5) Coerce Volume to numeric (basic version: strip non-numeric)\n",
    "if \"Volume\" in data.columns:\n",
    "    data[\"Volume\"] = pd.to_numeric(data[\"Volume\"].astype(str).map(to_numeric_str), errors=\"coerce\")\n",
    "\n",
    "# Optional: if your Volume has K/M/B suffixes (e.g., 12K, 3.5M, 1.2B) and you want correct scaling:\n",
    "def parse_volume(v):\n",
    "    v = str(v).strip()\n",
    "    if not v or v.lower() == \"nan\":\n",
    "        return float(\"nan\")\n",
    "    mult = 1\n",
    "    if v[-1] in \"KkMmBb\":\n",
    "        suffix = v[-1].upper()\n",
    "        v = to_numeric_str(v[:-1])\n",
    "        mult = {\"K\": 1_000, \"M\": 1_000_000, \"B\": 1_000_000_000}[suffix]\n",
    "    else:\n",
    "        v = to_numeric_str(v)\n",
    "    return float(v) * mult if v else float(\"nan\")\n",
    "if \"Volume\" in data.columns:\n",
    "    data[\"Volume\"] = data[\"Volume\"].map(parse_volume)\n",
    "\n",
    "# 6) Drop rows where any of OHLC are missing (mplfinance needs valid OHLC rows)\n",
    "if ohlc_cols:\n",
    "    data = data.dropna(subset=ohlc_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confirm Cleaned Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n*1* The number of observations and variables\")\n",
    "display(data.shape)\n",
    "\n",
    "print(\"\\n*2* The data types:\")\n",
    "display(data.info())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n*3* The summary of the numeric columns:\")\n",
    "display(data.describe())\n",
    "\n",
    "print(\"\\n*4* The whole dataset:\")\n",
    "display(data)\n",
    "\n",
    "print(\"\\n*5* The first 5 rows in the dataset:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\n*6* The last 5 rows in the dataset:\")\n",
    "display(data.tail())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Forecast"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.plot(data[\"Close\"])\n",
    "plt.title(f\"Stock Price (Closing) Time Series\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If data is already a Series, it uses it directly.\n",
    "# If data is a DataFrame, it selects the Close column and converts it to float64.\n",
    "ts = data if isinstance(data, pd.Series) else data[\"Close\"].astype('float64')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exponential Smoothing\n",
    "\n",
    "Exponential smoothing is a time series forecasting method for univariate data. It is a weighted average of past observations, with the weights decaying exponentially as the observations get older.\n",
    "\n",
    "**Note:** Unlike ARIMA models, some exponential smoothing models are designed to handle non-stationary data directly by explicitly modeling trend and seasonality. This is why we can apply them to the original time series."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Simple Exponential Smoothing (SES)\n",
    "\n",
    "- It is normal for Simple Exponential Smoothing (SES) to produce a flat forecast (the same value for all future periods) because it does not model trend or seasonality—only the level. The forecast equals the last estimated level from the fitted model.\n",
    "- For time series with trend or seasonality, it is better to use Holt’s or Holt-Winters methods instead."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit SES model\n",
    "model_ses = SimpleExpSmoothing(ts, initialization_method=\"estimated\").fit()\n",
    "\n",
    "# Forecast next 63 periods\n",
    "predictions_ses = model_ses.forecast(63)\n",
    "\n",
    "# Create future index for next 63 periods (e.g., business days)\n",
    "future_index = pd.date_range(\n",
    "    start=ts.index[-1] + pd.offsets.BDay(1),\n",
    "    periods=63,\n",
    "    freq='B'\n",
    ")\n",
    "\n",
    "# Assign index to forecast\n",
    "predictions_ses = pd.Series(predictions_ses.values, index=future_index)\n",
    "\n",
    "# Plot observed and forecasted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed', alpha=0.7)\n",
    "plt.plot(predictions_ses, label='Simple Exponential Smoothing Forecast')\n",
    "plt.title('Simple Exponential Smoothing (Next 3 Months)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_table = pd.DataFrame(predictions_ses)\n",
    "display(predictions_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Double Exponential Smoothing (Holt's Method)\n",
    "\n",
    "Holt's method is an extension of SES that can handle time series data with a trend."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Python\n",
    "model_holt = Holt(ts, initialization_method=\"estimated\").fit()\n",
    "predictions_holt = model_holt.forecast(63)\n",
    "\n",
    "future_index = pd.date_range(\n",
    "    start=ts.index[-1] + pd.offsets.BDay(1),  # Use business days if daily data\n",
    "    periods=63,\n",
    "    freq='B'\n",
    ")\n",
    "predictions_holt = pd.Series(predictions_holt.values, index=future_index)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed', alpha=0.7)\n",
    "plt.plot(predictions_holt, label=\"Holt's Forecast\")\n",
    "plt.title(\"Double Exponential Smoothing (Next Quarter)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_table = pd.DataFrame(predictions_holt)\n",
    "display(predictions_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Triple Exponential Smoothing (Holt-Winters Method)\n",
    "\n",
    "The Holt-Winters method extends Holt's method to capture seasonality."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Python\n",
    "model_hw = ExponentialSmoothing(ts, seasonal_periods=12, trend='add', seasonal='add', initialization_method=\"estimated\").fit()\n",
    "predictions_hw = model_hw.forecast(63)\n",
    "\n",
    "future_index = pd.date_range(\n",
    "    start=ts.index[-1] + pd.offsets.BDay(1),\n",
    "    periods=63,  # 63 business days ≈ 3 months\n",
    "    freq='B'\n",
    ")\n",
    "predictions_hw = pd.Series(predictions_hw.values, index=future_index)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed')\n",
    "plt.plot(predictions_hw, label='Holt-Winters Forecast')\n",
    "plt.title('Triple Exponential Smoothing (Holt-Winters Method)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_table = pd.DataFrame(predictions_hw)\n",
    "display(predictions_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ARIMA\n",
    "\n",
    "ARIMA stands for AutoRegressive Integrated Moving Average. It is a class of models that explains a given time series based on its own past values, that is, its own lags and the lagged forecast errors.\n",
    "\n",
    "An ARIMA model is characterized by 3 parameters: (p, d, q).\n",
    "\n",
    "* **p:** The number of lag observations included in the model (lag order).\n",
    "* **d:** The number of times that the raw observations are differenced (degree of differencing).\n",
    "* **q:** The size of the moving average window (order of moving average).\n",
    "\n",
    "**Important Note:** You might wonder why we are using the original `data` which is non-stationary. The `statsmodels` implementation of ARIMA is smart. The `d` parameter in the `order=(p,d,q)` argument tells the model how many times to difference the data. So, the model takes care of the differencing internally. This is the standard and recommended way to use ARIMA models in `statsmodels`, as it simplifies the process and automatically returns the forecasts on the original scale."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit ARIMA model on the closing price series\n",
    "model_arima = ARIMA(ts, order=(1, 1, 1)).fit()\n",
    "predictions_arima = model_arima.forecast(steps=63)\n",
    "\n",
    "# Create a future index for the next 63 business days\n",
    "future_index = pd.date_range(\n",
    "    start=ts.index[-1] + pd.offsets.BDay(1),\n",
    "    periods=63,\n",
    "    freq='B'\n",
    ")\n",
    "predictions_arima = pd.Series(predictions_arima.values, index=future_index)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed')\n",
    "plt.plot(predictions_arima, label='ARIMA Forecast')\n",
    "plt.title('ARIMA Forecast (Next 3 Months)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_table = pd.DataFrame(predictions_arima)\n",
    "display(predictions_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SARIMA\n",
    "\n",
    "SARIMA (Seasonal ARIMA) is an extension of ARIMA that supports time series data with a seasonal component.\n",
    "\n",
    "A SARIMA model is characterized by 7 parameters: (p, d, q) x (P, D, Q, m).\n",
    "\n",
    "* **(p, d, q):** The non-seasonal parameters of the ARIMA model.\n",
    "* **(P, D, Q, m):** The seasonal parameters of the model, where 'm' is the number of time steps for a single seasonal period.\n",
    "\n",
    "Just like with ARIMA, we use the original time series here and let the model handle the differencing (both seasonal and non-seasonal) for us via the `d` and `D` parameters."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_sarima = SARIMAX(ts, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)).fit()\n",
    "predictions_sarima = model_sarima.forecast(steps=63)\n",
    "\n",
    "future_index = pd.date_range(\n",
    "    start=ts.index[-1] + pd.offsets.BDay(1),\n",
    "    periods=63,\n",
    "    freq='B'\n",
    ")\n",
    "predictions_sarima = pd.Series(predictions_sarima.values, index=future_index)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed')\n",
    "plt.plot(predictions_sarima, label='SARIMA Forecast')\n",
    "plt.title('SARIMA Forecast (Next 3 Months)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_table = pd.DataFrame(predictions_sarima)\n",
    "display(predictions_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prophet\n",
    "\n",
    "- Resetting the index is necessary when preparing data for Prophet because Prophet expects a DataFrame with two columns:\n",
    "    - ds (date/time column)\n",
    "    - y (value column)\n",
    "- If your time series uses the date as the index, resetting the index moves the date from the index into a column, making it accessible for Prophet.\n",
    "- This ensures the DataFrame has the required structure for Prophet to fit and forecast."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare data for Prophet\n",
    "df_prophet = ts.reset_index().rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "df_prophet.columns = ['ds', 'y']  # Ensure correct column names\n",
    "\n",
    "# Define holidays\n",
    "holidays = pd.DataFrame({\n",
    "    'holiday': ['new_year', 'christmas'],\n",
    "    'ds': pd.to_datetime(['2023-01-01', '2023-12-25'])\n",
    "})\n",
    "\n",
    "# Initialize Prophet with custom settings\n",
    "tf_prophet_model = Prophet(\n",
    "    holidays=holidays,\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False\n",
    ")\n",
    "tf_prophet_model.add_seasonality(name='quarterly', period=63, fourier_order=5)\n",
    "\n",
    "# Fit Prophet model\n",
    "tf_prophet_model.fit(df_prophet)\n",
    "\n",
    "# Create future DataFrame for the next 63 business days\n",
    "future = tf_prophet_model.make_future_dataframe(periods=63, freq='B')\n",
    "\n",
    "# Forecast\n",
    "forecast = tf_prophet_model.predict(future)\n",
    "\n",
    "# Plot forecast\n",
    "fig = tf_prophet_model.plot(forecast)\n",
    "plt.title('Prophet Forecast (Next 3 Months)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The DataFrame returned by Prophet's predict method contains the following columns:\n",
    "    - ds: The datestamp for each prediction.\n",
    "    - yhat: The predicted value (forecast) for each date.\n",
    "    - yhat_lower: The lower bound of the uncertainty interval for the forecast.\n",
    "    - yhat_upper: The upper bound of the uncertainty interval for the forecast.\n",
    "    - trend: The estimated trend component.\n",
    "    - trend_lower / trend_upper: Lower/upper bounds for the trend.\n",
    "    - additive_terms: Sum of all additive components (seasonality, holidays, etc.).\n",
    "    - additive_terms_lower / additive_terms_upper: Bounds for additive terms.\n",
    "    - weekly, yearly, daily, holidays: Seasonality and holiday components (if modeled).\n",
    "    - weekly_lower / weekly_upper, etc.: Bounds for each seasonal/holiday component.\n",
    "\n",
    "- Not all columns appear if certain components (like holidays or specific seasonalities) are not included in the model. The most important column for predictions is yhat."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions_table = pd.DataFrame(forecast)\n",
    "display(predictions_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LSTM baseline\n",
    "LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) designed to learn from sequential data, making it well-suited for time series forecasting.\n",
    "TensorFlow (with Keras) is a widely used framework for building LSTM and other neural networks. It is a good starting point for time series forecasting with LSTM.\n",
    "Below is a simple baseline example for LSTM time series forecasting using TensorFlow/Keras:\n",
    "Explanation:\n",
    "The code scales the data, creates input sequences, splits into train/test, builds a simple LSTM model, trains it, and forecasts future values."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Baseline LSTM using ts ---\n",
    "window_size = 12\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_ts = scaler.fit_transform(ts.values.reshape(-1, 1))\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size, 0])\n",
    "        y.append(data[i+window_size, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_ts, window_size)\n",
    "X = X.reshape((X.shape[0], window_size, 1))\n",
    "\n",
    "# Build and train LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='tanh', input_shape=(window_size, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, y, epochs=30, batch_size=16, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Forecast next 63 business days\n",
    "last_sequence = scaled_ts[-window_size:].reshape(1, window_size, 1)\n",
    "future_preds_scaled = []\n",
    "for _ in range(63):\n",
    "    pred = model.predict(last_sequence, verbose=0)\n",
    "    future_preds_scaled.append(pred[0, 0])\n",
    "    last_sequence = np.concatenate([last_sequence[:, 1:, :], pred.reshape(1, 1, 1)], axis=1)\n",
    "\n",
    "future_preds = scaler.inverse_transform(np.array(future_preds_scaled).reshape(-1, 1)).flatten()\n",
    "future_index = pd.date_range(start=ts.index[-1] + pd.offsets.BDay(1), periods=63, freq='B')\n",
    "\n",
    "# --- Plot the forecast ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed')\n",
    "plt.plot(future_index, future_preds, label='LSTM Forecast (Next 63 Business Days)')\n",
    "plt.title('LSTM Forecast (Next 63 Business Days)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Display forecasted data in a table ---\n",
    "forecast_table = pd.DataFrame({'LSTM Forecast': future_preds}, index=future_index)\n",
    "display(forecast_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM with hyperparameter tuning"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Data Preparation ---\n",
    "window_size = 12\n",
    "scaler = MinMaxScaler()\n",
    "scaled_ts = scaler.fit_transform(ts.values.reshape(-1, 1))\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size, 0])\n",
    "        y.append(data[i+window_size, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_ts, window_size)\n",
    "X = X.reshape((X.shape[0], window_size, 1))\n",
    "\n",
    "# --- Hyperparameter Grid ---\n",
    "hyperparams = [\n",
    "    {'units': 32, 'lr': 0.01, 'epochs': 40, 'batch': 16},\n",
    "    {'units': 64, 'lr': 0.005, 'epochs': 50, 'batch': 32},\n",
    "    {'units': 100, 'lr': 0.001, 'epochs': 60, 'batch': 16}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for params in hyperparams:\n",
    "    model = Sequential([\n",
    "        LSTM(params['units'], activation='tanh', input_shape=(window_size, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=params['lr']), loss='mse')\n",
    "    model.fit(X, y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=0)\n",
    "    # Forecast next 63 business days\n",
    "    last_seq = scaled_ts[-window_size:].reshape(1, window_size, 1)\n",
    "    preds_scaled = []\n",
    "    for _ in range(63):\n",
    "        pred = model.predict(last_seq, verbose=0)\n",
    "        preds_scaled.append(pred[0, 0])\n",
    "        last_seq = np.concatenate([last_seq[:, 1:, :], pred.reshape(1, 1, 1)], axis=1)\n",
    "    preds = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()\n",
    "    key = f\"LSTM_units={params['units']}_lr={params['lr']}_ep={params['epochs']}_batch={params['batch']}\"\n",
    "    results[key] = preds\n",
    "\n",
    "future_index = pd.date_range(start=ts.index[-1] + pd.offsets.BDay(1), periods=63, freq='B')\n",
    "results_df = pd.DataFrame(results, index=future_index)\n",
    "\n",
    "# --- Display Results ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed')\n",
    "for col in results_df.columns:\n",
    "    plt.plot(results_df.index, results_df[col], label=col)\n",
    "plt.title('LSTM Forecasts (Hyperparameter Tuning, Next 63 Business Days)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(results_df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LSTM with hyperparameter tuning using your GPU with TensorFlow\n",
    "\n",
    "To ensure you have compatible NVIDIA drivers and CUDA/cuDNN installed for TensorFlow GPU support:\n",
    "1. Check your GPU model:\n",
    "Run nvidia-smi in the terminal to see your GPU and driver version.\n",
    "2. Install the latest NVIDIA driver:\n",
    "Download and install the latest driver for your GPU from the NVIDIA Driver Downloads page.\n",
    "3. Check TensorFlow compatibility:\n",
    "Refer to the TensorFlow GPU support guide for the required CUDA and cuDNN versions for your TensorFlow release.\n",
    "4. Install CUDA Toolkit:\n",
    "Download the correct CUDA version from the NVIDIA CUDA Toolkit.\n",
    "5. Install cuDNN:\n",
    "Download the matching cuDNN version from the NVIDIA cuDNN Archive.\n",
    "6. Add CUDA and cuDNN to your PATH:\n",
    "Add the bin and lib folders of CUDA and cuDNN to your system environment variables.\n",
    "7. Verify installation:\n",
    "Run the following in Python to check if TensorFlow detects your GPU:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Number of usable GPUs Available:\", len(tf.config.list_physical_devices('GPU')))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0, \"No usable GPUs found.\"\n",
    "\n",
    "window_size = 12\n",
    "scaler = MinMaxScaler()\n",
    "scaled_ts = scaler.fit_transform(ts.values.reshape(-1, 1))\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size, 0])\n",
    "        y.append(data[i+window_size, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_ts, window_size)\n",
    "X = X.reshape((X.shape[0], window_size, 1))\n",
    "\n",
    "hyperparams = [\n",
    "    {'units': 32, 'lr': 0.01, 'epochs': 40, 'batch': 16},\n",
    "    {'units': 64, 'lr': 0.005, 'epochs': 50, 'batch': 32},\n",
    "    {'units': 100, 'lr': 0.001, 'epochs': 60, 'batch': 16}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "with tf.device('/GPU:0'):\n",
    "    for params in hyperparams:\n",
    "        model = Sequential([\n",
    "            LSTM(params['units'], activation='tanh', input_shape=(window_size, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=params['lr']), loss='mse')\n",
    "        model.fit(X, y, epochs=params['epochs'], batch_size=params['batch'], validation_split=0.1, verbose=0)\n",
    "        # Forecast next 63 business days\n",
    "        last_seq = scaled_ts[-window_size:].reshape(1, window_size, 1)\n",
    "        preds_scaled = []\n",
    "        for _ in range(63):\n",
    "            pred = model.predict(last_seq, verbose=0)\n",
    "            preds_scaled.append(pred[0, 0])\n",
    "            last_seq = np.concatenate([last_seq[:, 1:, :], pred.reshape(1, 1, 1)], axis=1)\n",
    "        preds = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()\n",
    "        key = f\"LSTM_units={params['units']}_lr={params['lr']}_ep={params['epochs']}_batch={params['batch']}\"\n",
    "        results[key] = preds\n",
    "\n",
    "future_index = pd.date_range(start=ts.index[-1] + pd.offsets.BDay(1), periods=63, freq='B')\n",
    "results_df = pd.DataFrame(results, index=future_index)\n",
    "display(results_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts, label='Observed')\n",
    "for col in results_df.columns:\n",
    "    plt.plot(results_df.index, results_df[col], label=col)\n",
    "plt.title('LSTM Forecasts (GPU, Hyperparameter Tuning, Next 63 Business Days)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
